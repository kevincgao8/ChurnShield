{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb8ec97-23ee-4e16-a7e7-b3c183267664",
   "metadata": {},
   "source": [
    "## Importing all the sources of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46aee78e-751e-443a-b31d-13e63e7c80b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading billing data from s3://my-churnshield-data/Data Sources/billing_data.csv...\n",
      "Billing Data Sample:\n",
      "                                  Billing_ID     UserID SubscriptionPlan  \\\n",
      "0  BILL-27eba3ee-e298-4348-98ed-9fa8474ff727  USER-8359            Basic   \n",
      "1  BILL-6682bc84-8806-42b5-ab80-df08b7f5b5ca  USER-3216       Enterprise   \n",
      "2  BILL-c3d9c47b-032f-4c7e-8322-6549ee14b4c8  USER-5794       Enterprise   \n",
      "3  BILL-44884268-8956-44a1-ab2a-37fc9277f0c9  USER-6822       Enterprise   \n",
      "4  BILL-edecccfc-da79-423f-b17c-f8952bbd559f  USER-1518       Enterprise   \n",
      "\n",
      "   Amount Currency             TransactionDate  PaymentMethod PaymentStatus  \n",
      "0   30.39      USD  2024-12-28T20:41:39.615550  Wire Transfer       Pending  \n",
      "1  145.33      USD  2024-11-28T00:04:24.702024         PayPal        Failed  \n",
      "2  491.75      USD  2024-08-22T14:09:02.394898  Wire Transfer        Failed  \n",
      "3  185.83      USD  2024-03-22T16:31:45.238658    Credit Card       Pending  \n",
      "4  183.16      USD  2025-01-06T13:06:24.559926  Wire Transfer     Completed  \n",
      "\n",
      "\n",
      "Loading hubspot data from s3://my-churnshield-data/Data Sources/hubspot_data.csv...\n",
      "Hubspot Data Sample:\n",
      "                                HubSpot_ID                   CreatedOn  \\\n",
      "0  HS-1c5e17ac-28ad-4771-b385-62a710336f69  2024-08-27T21:03:33.090348   \n",
      "1  HS-451b47c1-0ed0-4da4-93a4-f1b3a8ebf219  2025-01-03T04:35:43.780510   \n",
      "2  HS-f2ed3f7c-a4ca-40f1-aba3-a2ca6ba1bf6e  2024-07-10T06:11:38.096262   \n",
      "3  HS-54afff7f-a327-4fb5-a8b9-60b9c036d567  2025-01-04T15:03:45.793348   \n",
      "4  HS-526b530f-5c09-407d-9c33-673fd4a516e2  2024-11-01T23:27:35.954706   \n",
      "\n",
      "  FirstName  LastName                   Email            LifecycleStage  \\\n",
      "0     Molly  Mitchell     lrhodes@example.net                      Lead   \n",
      "1  Kimberly   Krueger   matthew74@example.net               Opportunity   \n",
      "2    Steven    Oliver    bonnie60@example.org  Marketing Qualified Lead   \n",
      "3      Mark      Rush    randrews@example.net      Sales Qualified Lead   \n",
      "4       Amy    Chaney  colinblake@example.org                      Lead   \n",
      "\n",
      "                         Website  LeadScore  \n",
      "0  https://edwards-robinson.biz/         39  \n",
      "1       http://www.gonzalez.com/         15  \n",
      "2             http://conrad.biz/          9  \n",
      "3        http://www.bullock.com/          9  \n",
      "4            https://davies.com/         91  \n",
      "\n",
      "\n",
      "Loading salesforce data from s3://my-churnshield-data/Data Sources/salesforce_data.csv...\n",
      "Salesforce Data Sample:\n",
      "                             Salesforce_ID                 CreatedDate  \\\n",
      "0  SF-922e8ed5-0bf3-4b2f-8b01-46504b5695b9  2024-08-25T13:56:14.111596   \n",
      "1  SF-d4eee776-5278-4335-90fd-a18a57565da6  2024-12-10T20:11:12.933288   \n",
      "2  SF-eab2ec1f-a41d-4c16-a727-3a821d381ac0  2024-08-06T08:45:38.398177   \n",
      "3  SF-a679d3c4-e9db-4573-a7b5-10f4b28150c8  2024-06-03T16:55:19.829209   \n",
      "4  SF-4fcb66c4-eee6-4926-9d1d-f529bffa916d  2025-01-05T03:47:36.146496   \n",
      "\n",
      "   FirstName LastName                         Email                  Phone  \\\n",
      "0   Patricia  Trevino      robertsjacob@example.org  +1-266-594-2273x09353   \n",
      "1      Ricky    Brady         michael39@example.net          (249)214-2262   \n",
      "2    Annette    Hines  ronaldrichardson@example.net  001-488-964-5881x1286   \n",
      "3    Annette  Hoffman         william67@example.net   +1-396-457-5053x8910   \n",
      "4  Cassandra   Cortez        shawnasims@example.org  001-481-634-0644x9584   \n",
      "\n",
      "           Company  LeadSource         Status  \n",
      "0     Thompson Ltd  Trade Show      Converted  \n",
      "1       Cooper Ltd   Cold Call      Nurturing  \n",
      "2      Salinas PLC   Cold Call        Working  \n",
      "3  Williams-Curtis    Referral            New  \n",
      "4       Taylor Ltd   Cold Call  Closed - Lost  \n",
      "\n",
      "\n",
      "Loading zendesk data from s3://my-churnshield-data/Data Sources/zendesk_data.csv...\n",
      "Zendesk Data Sample:\n",
      "                                 Ticket_ID                 SubmittedAt  \\\n",
      "0  ZD-b59bc9dc-a911-4ed5-b286-47dcd465d527  2024-07-03T05:21:11.206803   \n",
      "1  ZD-7bec3e64-dad6-45f2-8b95-f12d6fed915a  2024-12-08T22:54:35.805105   \n",
      "2  ZD-cf900bc2-7663-43fc-b425-4e2f7611127c  2024-05-07T00:16:50.778418   \n",
      "3  ZD-0d561782-be40-4f1d-afe8-f5a0dd95a32b  2025-01-12T16:50:56.347338   \n",
      "4  ZD-1d6c76a9-4761-4ec0-b020-36b127a36181  2024-09-03T14:44:16.091109   \n",
      "\n",
      "                    UpdatedAt     UserID  \\\n",
      "0  2024-07-03T05:21:11.206803  USER-7191   \n",
      "1  2025-01-05T22:54:35.805105  USER-4471   \n",
      "2  2024-05-13T00:16:50.778418  USER-7103   \n",
      "3  2025-01-26T16:50:56.347338  USER-6479   \n",
      "4  2024-09-06T14:44:16.091109  USER-9602   \n",
      "\n",
      "                           TicketSubject  \\\n",
      "0  Trouble box center ok accept science.   \n",
      "1      Always attorney individual scene.   \n",
      "2            How safe so claim left own.   \n",
      "3            Imagine face evidence road.   \n",
      "4  Direction leave technology important.   \n",
      "\n",
      "                                   TicketDescription  Status Priority  \\\n",
      "0    Worry foot back whatever recently police civil.    Open     High   \n",
      "1  Matter mission must event. Four well member se...  Solved      Low   \n",
      "2                             Will wait today begin.  Solved   Normal   \n",
      "3  Positive hour gas require me watch boy. Despit...    Open   Normal   \n",
      "4  Understand area seek. Accept soon whose finish...    Open     High   \n",
      "\n",
      "                  Agent  \n",
      "0         Laura Edwards  \n",
      "1       Michael Harrell  \n",
      "2  Alejandra Fitzgerald  \n",
      "3             Amy Ponce  \n",
      "4         Frank Nichols  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "# Define S3 bucket and the folder where data source CSVs are stored\n",
    "bucket = \"my-churnshield-data\"\n",
    "folder = \"Data Sources/\"\n",
    "\n",
    "# Define a dictionary mapping source names to their CSV filenames\n",
    "files = {\n",
    "    \"billing\": \"billing_data.csv\",\n",
    "    \"hubspot\": \"hubspot_data.csv\",\n",
    "    \"salesforce\": \"salesforce_data.csv\",\n",
    "    \"zendesk\": \"zendesk_data.csv\"\n",
    "}\n",
    "\n",
    "# Create an S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Loop over the files and import each CSV into a Pandas DataFrame\n",
    "for source, filename in files.items():\n",
    "    key = folder + filename\n",
    "    print(f\"Loading {source} data from s3://{bucket}/{key}...\")\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    data = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "    # Read CSV; adjust header=None and names=[...] if your CSVs lack headers\n",
    "    df = pd.read_csv(StringIO(data))\n",
    "    dataframes[source] = df\n",
    "    print(f\"{source.capitalize()} Data Sample:\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c47fa-23c3-4900-8d34-41d2eb8fe965",
   "metadata": {},
   "source": [
    "## Salesforce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60795ce8-73b6-437f-b1d8-ed2ad0334558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Faker\n",
      "  Using cached Faker-36.1.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tzdata in /opt/conda/lib/python3.11/site-packages (from Faker) (2025.1)\n",
      "Using cached Faker-36.1.1-py3-none-any.whl (1.9 MB)\n",
      "Installing collected packages: Faker\n",
      "Successfully installed Faker-36.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6730ca41-9a8c-488d-8fe0-933935e222a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully synthetic Salesforce-like data created: salesforce_data_synthetic.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "num_rows = 100  # synthetic rows \n",
    "\n",
    "synthetic_data = []\n",
    "for _ in range(num_rows):\n",
    "    salesforce_id = \"SF-\" + str(uuid.uuid4())[:8]  # short random ID\n",
    "    created_date = fake.date_time_between(start_date=\"-2y\", end_date=\"now\").isoformat()\n",
    "    first_name = fake.first_name()\n",
    "    last_name = fake.last_name()\n",
    "    email = fake.email()\n",
    "    phone = fake.phone_number()\n",
    "    company = fake.company()\n",
    "    lead_source = random.choice([\"Trade Show\", \"Cold Call\", \"Referral\", \"Web\", \"Partner\"])\n",
    "    status = random.choice([\"New\", \"Working\", \"Nurturing\", \"Converted\", \"Closed - Lost\"])\n",
    "    churn = random.choice([0, 1])  # random churn label\n",
    "\n",
    "    synthetic_data.append([\n",
    "        salesforce_id,\n",
    "        created_date,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        email,\n",
    "        phone,\n",
    "        company,\n",
    "        lead_source,\n",
    "        status,\n",
    "        churn\n",
    "    ])\n",
    "\n",
    "columns = [\n",
    "    \"Salesforce_ID\", \"CreatedDate\", \"FirstName\", \"LastName\",\n",
    "    \"Email\", \"Phone\", \"Company\", \"LeadSource\", \"Status\", \"Churn\"\n",
    "]\n",
    "df_synthetic = pd.DataFrame(synthetic_data, columns=columns)\n",
    "\n",
    "# Save to CSV\n",
    "df_synthetic.to_csv(\"salesforce_data_synthetic.csv\", index=False)\n",
    "print(\"Fully synthetic Salesforce-like data created: salesforce_data_synthetic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147459c0-973c-4aba-b8a7-01fd298b1577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample:\n",
      "  Salesforce_ID                 CreatedDate FirstName   LastName  \\\n",
      "0   SF-f135acaf  2024-12-31T16:39:51.357132   Melissa  Rodriguez   \n",
      "1   SF-f0a2cd3c  2023-03-14T20:05:52.240183    Adrian      Chang   \n",
      "2   SF-89c87ae8  2024-10-13T18:24:38.363084   Michael      Craig   \n",
      "3   SF-2cb14743  2024-07-28T15:27:55.404905     Holly     Larson   \n",
      "4   SF-1906782f  2023-03-03T19:16:57.754639      Lisa      Allen   \n",
      "\n",
      "                         Email                Phone           Company  \\\n",
      "0     burgesscraig@example.org  (659)733-3378x70650       Maxwell Ltd   \n",
      "1        seanmunoz@example.org        (985)734-3832       Wong-Becker   \n",
      "2     douglaskaren@example.org    399-626-4173x8661   Fisher and Sons   \n",
      "3        vincent27@example.com         751.484.6636      Hamilton LLC   \n",
      "4  chambersmatthew@example.net         273.690.7188  Kaufman-Cardenas   \n",
      "\n",
      "   LeadSource     Status  Churn  \n",
      "0    Referral    Working      0  \n",
      "1         Web    Working      1  \n",
      "2         Web  Converted      1  \n",
      "3  Trade Show        New      0  \n",
      "4         Web  Nurturing      0  \n",
      "\n",
      "Encoded Features Sample:\n",
      "   LeadSource_Cold Call  LeadSource_Partner  LeadSource_Referral  \\\n",
      "0                 False               False                 True   \n",
      "1                 False               False                False   \n",
      "2                 False               False                False   \n",
      "3                 False               False                False   \n",
      "4                 False               False                False   \n",
      "\n",
      "   LeadSource_Trade Show  LeadSource_Web  Status_Closed - Lost  \\\n",
      "0                  False           False                 False   \n",
      "1                  False            True                 False   \n",
      "2                  False            True                 False   \n",
      "3                   True           False                 False   \n",
      "4                  False            True                 False   \n",
      "\n",
      "   Status_Converted  Status_New  Status_Nurturing  Status_Working  \n",
      "0             False       False             False            True  \n",
      "1             False       False             False            True  \n",
      "2              True       False             False           False  \n",
      "3             False        True             False           False  \n",
      "4             False       False              True           False  \n",
      "Training columns: ['LeadSource_Cold Call', 'LeadSource_Partner', 'LeadSource_Referral', 'LeadSource_Trade Show', 'LeadSource_Web', 'Status_Closed - Lost', 'Status_Converted', 'Status_New', 'Status_Nurturing', 'Status_Working']\n",
      "\n",
      "Model Accuracy: 0.5\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.62      0.62        13\n",
      "           1       0.29      0.29      0.29         7\n",
      "\n",
      "    accuracy                           0.50        20\n",
      "   macro avg       0.45      0.45      0.45        20\n",
      "weighted avg       0.50      0.50      0.50        20\n",
      "\n",
      "Salesforce model saved to salesforce_model.joblib\n"
     ]
    }
   ],
   "source": [
    "## TRAINING THE MODEL\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# 1. Load synthetic data from CSV\n",
    "df = pd.read_csv(\"salesforce_data_synthetic.csv\")\n",
    "print(\"Data Sample:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Select features and target\n",
    "# We'll use 'LeadSource' and 'Status' as features, and 'Churn' as the target.\n",
    "features = df[[\"LeadSource\", \"Status\"]]\n",
    "target = df[\"Churn\"]\n",
    "\n",
    "# 3. One-hot encode the categorical features\n",
    "features_encoded = pd.get_dummies(features)\n",
    "print(\"\\nEncoded Features Sample:\")\n",
    "print(features_encoded.head())\n",
    "# Save the columns as a list\n",
    "training_columns = list(features_encoded.columns)\n",
    "print(\"Training columns:\", training_columns)\n",
    "\n",
    "# Store them in a file or Python variable for reuse\n",
    "import json\n",
    "with open(\"salesforce_columns.json\", \"w\") as f:\n",
    "    json.dump(training_columns, f)\n",
    "\n",
    "# 4. Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_encoded, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Train a RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nModel Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 7. Save the trained model to a file for later use\n",
    "joblib.dump(clf, \"salesforce_model.joblib\")\n",
    "print(\"Salesforce model saved to salesforce_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f7b72f-2afd-4cc4-a37d-e474e61ded99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "#SageMaker endpoint name\n",
    "ENDPOINT_NAME = \"sagemaker-xgboost-2025-02-20-03-57-21-973\"\n",
    "\n",
    "# Create a SageMaker runtime client\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=\"us-east-2\")\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        # Parse the request body (assumes application/json content type)\n",
    "        body = json.loads(event.get(\"body\", \"{}\"))\n",
    "        features = body.get(\"features\")  # e.g., a list of feature values\n",
    "\n",
    "        if not features:\n",
    "            return {\n",
    "                \"statusCode\": 400,\n",
    "                \"body\": json.dumps({\"error\": \"Missing 'features' in request\"})\n",
    "            }\n",
    "\n",
    "        # Convert feature list to a CSV-formatted string (as expected by the model)\n",
    "        csv_payload = \",\".join(map(str, features))\n",
    "\n",
    "        # Invoke the SageMaker endpoint with the CSV payload\n",
    "        response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=ENDPOINT_NAME,\n",
    "            ContentType=\"text/csv\",\n",
    "            Body=csv_payload\n",
    "        )\n",
    "\n",
    "        # Read the prediction response from the model\n",
    "        prediction = response[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "        # Return the prediction as JSON\n",
    "        return {\n",
    "            \"statusCode\": 200,\n",
    "            \"body\": json.dumps({\n",
    "                \"prediction\": prediction,\n",
    "                \"features\": features\n",
    "            })\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"statusCode\": 500,\n",
    "            \"body\": json.dumps({\"error\": str(e)})\n",
    "        }\n",
    "\n",
    "\n",
    "# Create a SageMaker runtime client\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=\"us-east-2\")\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        # Parse the request body (assumes application/json content type)\n",
    "        body = json.loads(event.get(\"body\", \"{}\"))\n",
    "        features = body.get(\"features\")  # e.g., a list of feature values\n",
    "\n",
    "        if not features:\n",
    "            return {\n",
    "                \"statusCode\": 400,\n",
    "                \"body\": json.dumps({\"error\": \"Missing 'features' in request\"})\n",
    "            }\n",
    "\n",
    "        # Convert feature list to a CSV-formatted string (as expected by the model)\n",
    "        csv_payload = \",\".join(map(str, features))\n",
    "\n",
    "        # Invoke the SageMaker endpoint with the CSV payload\n",
    "        response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=ENDPOINT_NAME,\n",
    "            ContentType=\"text/csv\",\n",
    "            Body=csv_payload\n",
    "        )\n",
    "\n",
    "        # Read the prediction response from the model\n",
    "        prediction = response[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "        # Return the prediction as JSON\n",
    "        return {\n",
    "            \"statusCode\": 200,\n",
    "            \"body\": json.dumps({\n",
    "                \"prediction\": prediction,\n",
    "                \"features\": features\n",
    "            })\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"statusCode\": 500,\n",
    "            \"body\": json.dumps({\"error\": str(e)})\n",
    "        }\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7570f2d8-6880-4655-ac3f-3f84926fd427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='salesforce_model.joblib' target='_blank'>salesforce_model.joblib</a><br>"
      ],
      "text/plain": [
       "/home/sagemaker-user/salesforce_model.joblib"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# For the raw .joblib file:\n",
    "display(FileLink('salesforce_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8398e0-c622-46dd-9f82-dffde3ecfb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Salesforce Data Sample:\n",
      "                             Salesforce_ID                 CreatedDate  \\\n",
      "0  SF-922e8ed5-0bf3-4b2f-8b01-46504b5695b9  2024-08-25T13:56:14.111596   \n",
      "1  SF-d4eee776-5278-4335-90fd-a18a57565da6  2024-12-10T20:11:12.933288   \n",
      "2  SF-eab2ec1f-a41d-4c16-a727-3a821d381ac0  2024-08-06T08:45:38.398177   \n",
      "3  SF-a679d3c4-e9db-4573-a7b5-10f4b28150c8  2024-06-03T16:55:19.829209   \n",
      "4  SF-4fcb66c4-eee6-4926-9d1d-f529bffa916d  2025-01-05T03:47:36.146496   \n",
      "\n",
      "   FirstName LastName                         Email                  Phone  \\\n",
      "0   Patricia  Trevino      robertsjacob@example.org  +1-266-594-2273x09353   \n",
      "1      Ricky    Brady         michael39@example.net          (249)214-2262   \n",
      "2    Annette    Hines  ronaldrichardson@example.net  001-488-964-5881x1286   \n",
      "3    Annette  Hoffman         william67@example.net   +1-396-457-5053x8910   \n",
      "4  Cassandra   Cortez        shawnasims@example.org  001-481-634-0644x9584   \n",
      "\n",
      "           Company  LeadSource         Status  \n",
      "0     Thompson Ltd  Trade Show      Converted  \n",
      "1       Cooper Ltd   Cold Call      Nurturing  \n",
      "2      Salinas PLC   Cold Call        Working  \n",
      "3  Williams-Curtis    Referral            New  \n",
      "4       Taylor Ltd   Cold Call  Closed - Lost  \n",
      "\n",
      "Encoded Client Features Sample:\n",
      "   LeadSource_Cold Call  LeadSource_Partner  LeadSource_Referral  \\\n",
      "0                 False               False                False   \n",
      "1                  True               False                False   \n",
      "2                  True               False                False   \n",
      "3                 False               False                 True   \n",
      "4                  True               False                False   \n",
      "\n",
      "   LeadSource_Trade Show  LeadSource_Web  Status_Closed - Lost  \\\n",
      "0                   True           False                 False   \n",
      "1                  False           False                 False   \n",
      "2                  False           False                 False   \n",
      "3                  False           False                 False   \n",
      "4                  False           False                  True   \n",
      "\n",
      "   Status_Converted  Status_New  Status_Nurturing  Status_Working  \n",
      "0              True       False             False           False  \n",
      "1             False       False              True           False  \n",
      "2             False       False             False            True  \n",
      "3             False        True             False           False  \n",
      "4             False       False             False           False  \n",
      "\n",
      "Columns from training (salesforce_columns.json):\n",
      "['LeadSource_Cold Call', 'LeadSource_Partner', 'LeadSource_Referral', 'LeadSource_Trade Show', 'LeadSource_Web', 'Status_Closed - Lost', 'Status_Converted', 'Status_New', 'Status_Nurturing', 'Status_Working']\n",
      "\n",
      "Reindexed Encoded Features (should have 10 columns):\n",
      "   LeadSource_Cold Call  LeadSource_Partner  LeadSource_Referral  \\\n",
      "0                 False               False                False   \n",
      "1                  True               False                False   \n",
      "2                  True               False                False   \n",
      "3                 False               False                 True   \n",
      "4                  True               False                False   \n",
      "\n",
      "   LeadSource_Trade Show  LeadSource_Web  Status_Closed - Lost  \\\n",
      "0                   True           False                 False   \n",
      "1                  False           False                 False   \n",
      "2                  False           False                 False   \n",
      "3                  False           False                 False   \n",
      "4                  False           False                  True   \n",
      "\n",
      "   Status_Converted  Status_New  Status_Nurturing  Status_Working  \n",
      "0              True       False             False           False  \n",
      "1             False       False              True           False  \n",
      "2             False       False             False            True  \n",
      "3             False        True             False           False  \n",
      "4             False       False             False           False  \n",
      "\n",
      "Predictions on Client Data:\n",
      "                             Salesforce_ID  LeadSource         Status  \\\n",
      "0  SF-922e8ed5-0bf3-4b2f-8b01-46504b5695b9  Trade Show      Converted   \n",
      "1  SF-d4eee776-5278-4335-90fd-a18a57565da6   Cold Call      Nurturing   \n",
      "2  SF-eab2ec1f-a41d-4c16-a727-3a821d381ac0   Cold Call        Working   \n",
      "3  SF-a679d3c4-e9db-4573-a7b5-10f4b28150c8    Referral            New   \n",
      "4  SF-4fcb66c4-eee6-4926-9d1d-f529bffa916d   Cold Call  Closed - Lost   \n",
      "\n",
      "   Predicted_Churn  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "3                1  \n",
      "4                1  \n",
      "\n",
      "Saved predictions to salesforce_data_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "## Using model to predict Salesforce churn\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Download client's Salesforce data from S3\n",
    "# -------------------------------\n",
    "bucket = \"my-churnshield-data\"\n",
    "key = \"Data Sources/salesforce_data.csv\"\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=\"us-east-2\")\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "data = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "df_clients = pd.read_csv(StringIO(data))\n",
    "print(\"Client Salesforce Data Sample:\")\n",
    "print(df_clients.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Preprocess the data to extract features\n",
    "# -------------------------------\n",
    "# We will use only 'LeadSource' and 'Status' as the model features.\n",
    "features = df_clients[[\"LeadSource\", \"Status\"]]\n",
    "\n",
    "# One-hot encode the categorical features\n",
    "features_encoded = pd.get_dummies(features)\n",
    "print(\"\\nEncoded Client Features Sample:\")\n",
    "print(features_encoded.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load the training columns from JSON\n",
    "# -------------------------------\n",
    "# This JSON file was created during training, containing the exact dummy columns.\n",
    "with open(\"salesforce_columns.json\", \"r\") as f:\n",
    "    expected_columns = json.load(f)\n",
    "\n",
    "print(\"\\nColumns from training (salesforce_columns.json):\")\n",
    "print(expected_columns)\n",
    "\n",
    "# Reindex the encoded DataFrame to have the expected columns, filling missing columns with 0\n",
    "features_encoded = features_encoded.reindex(columns=expected_columns, fill_value=0)\n",
    "print(\"\\nReindexed Encoded Features (should have\", len(expected_columns), \"columns):\")\n",
    "print(features_encoded.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Load the trained Salesforce-specific model\n",
    "# -------------------------------\n",
    "model = joblib.load(\"salesforce_model.joblib\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Generate churn predictions\n",
    "# -------------------------------\n",
    "predictions = model.predict(features_encoded)\n",
    "df_clients[\"Predicted_Churn\"] = predictions\n",
    "\n",
    "# Display a few predictions alongside key columns\n",
    "print(\"\\nPredictions on Client Data:\")\n",
    "print(df_clients[[\"Salesforce_ID\", \"LeadSource\", \"Status\", \"Predicted_Churn\"]].head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Save the DataFrame with predictions to a new CSV file\n",
    "# -------------------------------\n",
    "output_file = \"salesforce_data_with_predictions.csv\"\n",
    "df_clients.to_csv(output_file, index=False)\n",
    "print(\"\\nSaved predictions to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1572a712-fc79-448e-bb78-01ef14c13de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Configuration\n",
    "BUCKET = \"my-churnshield-data\"\n",
    "KEY = \"Predictions/salesforce_data_with_predictions.csv\"  \n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        # Create S3 client\n",
    "        s3_client = boto3.client(\"s3\", region_name=\"us-east-2\")\n",
    "        \n",
    "        # Get the CSV file from S3\n",
    "        obj = s3_client.get_object(Bucket=BUCKET, Key=KEY)\n",
    "        data = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "        \n",
    "        # Read CSV into a DataFrame\n",
    "        df = pd.read_csv(StringIO(data))\n",
    "        \n",
    "        # Optionally, can convert the DataFrame to JSON\n",
    "        # For example, converting the entire DataFrame to a list of dictionaries:\n",
    "        predictions = df.to_dict(orient=\"records\")\n",
    "        \n",
    "        # Return the predictions as a JSON response\n",
    "        return {\n",
    "            \"statusCode\": 200,\n",
    "            \"body\": json.dumps({\n",
    "                \"predictions\": predictions\n",
    "            }),\n",
    "            \"headers\": {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Access-Control-Allow-Origin\": \"*\"  \n",
    "            }\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"statusCode\": 500,\n",
    "            \"body\": json.dumps({\"error\": str(e)}),\n",
    "            \"headers\": {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Access-Control-Allow-Origin\": \"*\"\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edad3da-1f9c-4e10-aa19-117b48d8be62",
   "metadata": {},
   "source": [
    "## Hubspot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76c6a3b6-1100-4464-9318-d74b14c69231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Hubspot data with churn created: hubspot_data_synthetic_with_churn.csv\n"
     ]
    }
   ],
   "source": [
    "## SYNTHETIC DATA\n",
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "num_rows = 100\n",
    "lifecycle_stages = [\n",
    "    \"Lead\",\n",
    "    \"Opportunity\",\n",
    "    \"Marketing Qualified Lead\",\n",
    "    \"Sales Qualified Lead\",\n",
    "    \"Customer\",\n",
    "    \"Subscriber\"\n",
    "]\n",
    "\n",
    "synthetic_data = []\n",
    "for _ in range(num_rows):\n",
    "    hubspot_id = \"HS-\" + str(uuid.uuid4())[:8]\n",
    "    created_on = fake.date_time_between(start_date=\"-2y\", end_date=\"now\").isoformat()\n",
    "    first_name = fake.first_name()\n",
    "    last_name = fake.last_name()\n",
    "    email = fake.email()\n",
    "    lifecycle_stage = random.choice(lifecycle_stages)\n",
    "    website = fake.url()\n",
    "    \n",
    "    lead_score = random.randint(0, 100)\n",
    "\n",
    "    # lead_score looks indicative of churn here and we may be able to attribute it to churn. \n",
    "    # To create more \"real life\" data, let's go with that pattern here\n",
    "    # If lead_score < 30 => more likely to churn\n",
    "    if lead_score < 30:\n",
    "        churn = 1\n",
    "    else:\n",
    "        # Maybe 20% chance of churn otherwise\n",
    "        churn = 1 if random.random() < 0.2 else 0\n",
    "\n",
    "    synthetic_data.append([\n",
    "        hubspot_id,\n",
    "        created_on,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        email,\n",
    "        lifecycle_stage,\n",
    "        website,\n",
    "        lead_score,\n",
    "        churn\n",
    "    ])\n",
    "\n",
    "columns = [\n",
    "    \"HubSpot_ID\", \"CreatedOn\", \"FirstName\", \"LastName\",\n",
    "    \"Email\", \"LifecycleStage\", \"Website\", \"LeadScore\", \"Churn\"\n",
    "]\n",
    "\n",
    "df_synthetic = pd.DataFrame(synthetic_data, columns=columns)\n",
    "df_synthetic.to_csv(\"hubspot_data_synthetic.csv\", index=False)\n",
    "print(\"Synthetic Hubspot data with churn created: hubspot_data_synthetic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54066f29-6536-4c84-80e9-411d771d01bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample:\n",
      "    HubSpot_ID                   CreatedOn FirstName  LastName  \\\n",
      "0  HS-c62ae2ee  2023-07-10T15:01:28.152665    Jeremy     Sharp   \n",
      "1  HS-d3ce946e  2025-02-13T17:46:33.654093    Angela     Heath   \n",
      "2  HS-b386c5a7  2024-09-30T05:47:46.026878     Wendy    Bowers   \n",
      "3  HS-da142366  2024-12-17T21:01:46.441043     David  Thompson   \n",
      "4  HS-b5fac00d  2024-07-21T20:44:24.460131    Kristi      Reed   \n",
      "\n",
      "                         Email            LifecycleStage  \\\n",
      "0          maria28@example.com                  Customer   \n",
      "1     robertgarcia@example.com               Opportunity   \n",
      "2     cortezjustin@example.net  Marketing Qualified Lead   \n",
      "3           wellis@example.com  Marketing Qualified Lead   \n",
      "4  fergusondeborah@example.net                  Customer   \n",
      "\n",
      "                          Website  LeadScore  Churn  \n",
      "0              http://watson.org/         78      0  \n",
      "1         https://www.murray.com/         35      0  \n",
      "2  http://www.mcdaniel-gomez.org/         91      0  \n",
      "3       https://morris-lewis.com/         72      0  \n",
      "4            https://jackson.com/         85      0  \n",
      "\n",
      "Encoded Features Sample:\n",
      "   LifecycleStage_Customer  LifecycleStage_Lead  \\\n",
      "0                     True                False   \n",
      "1                    False                False   \n",
      "2                    False                False   \n",
      "3                    False                False   \n",
      "4                     True                False   \n",
      "\n",
      "   LifecycleStage_Marketing Qualified Lead  LifecycleStage_Opportunity  \\\n",
      "0                                    False                       False   \n",
      "1                                    False                        True   \n",
      "2                                     True                       False   \n",
      "3                                     True                       False   \n",
      "4                                    False                       False   \n",
      "\n",
      "   LifecycleStage_Sales Qualified Lead  LifecycleStage_Subscriber  LeadScore  \n",
      "0                                False                      False         78  \n",
      "1                                False                      False         35  \n",
      "2                                False                      False         91  \n",
      "3                                False                      False         72  \n",
      "4                                False                      False         85  \n",
      "Training columns: ['LifecycleStage_Customer', 'LifecycleStage_Lead', 'LifecycleStage_Marketing Qualified Lead', 'LifecycleStage_Opportunity', 'LifecycleStage_Sales Qualified Lead', 'LifecycleStage_Subscriber', 'LeadScore']\n",
      "\n",
      "Model Accuracy: 0.85\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89        13\n",
      "           1       0.83      0.71      0.77         7\n",
      "\n",
      "    accuracy                           0.85        20\n",
      "   macro avg       0.85      0.82      0.83        20\n",
      "weighted avg       0.85      0.85      0.85        20\n",
      "\n",
      "\n",
      "Hubspot model saved to hubspot_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# 1. Load synthetic Hubspot data from CSV\n",
    "df = pd.read_csv(\"hubspot_data_synthetic.csv\")\n",
    "print(\"Data Sample:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Select features and target\n",
    "# We'll use 'LifecycleStage' (categorical) and 'LeadScore' (numeric) as features, and 'Churn' as the target.\n",
    "features = df[[\"LifecycleStage\", \"LeadScore\"]]\n",
    "target = df[\"Churn\"]\n",
    "\n",
    "# 3. One-hot encode the 'LifecycleStage' column\n",
    "# Keep 'LeadScore' as is, since it's numeric\n",
    "lifecycle_encoded = pd.get_dummies(features[\"LifecycleStage\"], prefix=\"LifecycleStage\")\n",
    "\n",
    "# Combine the encoded lifecycle columns with the numeric LeadScore\n",
    "features_encoded = pd.concat([lifecycle_encoded, features[\"LeadScore\"]], axis=1)\n",
    "\n",
    "print(\"\\nEncoded Features Sample:\")\n",
    "print(features_encoded.head())\n",
    "\n",
    "# Save the columns as a list for inference-time reindexing\n",
    "training_columns = list(features_encoded.columns)\n",
    "print(\"Training columns:\", training_columns)\n",
    "\n",
    "# Store them in a JSON file for reuse during inference\n",
    "with open(\"hubspot_columns.json\", \"w\") as f:\n",
    "    json.dump(training_columns, f)\n",
    "\n",
    "# 4. Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_encoded, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Train a RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nModel Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 7. Save the trained model to a file for later use\n",
    "joblib.dump(clf, \"hubspot_model.joblib\")\n",
    "print(\"\\nHubspot model saved to hubspot_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1e8d4ae-db64-4a0a-b5d4-86006e2908c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='hubspot_model.joblib' target='_blank'>hubspot_model.joblib</a><br>"
      ],
      "text/plain": [
       "/home/sagemaker-user/hubspot_model.joblib"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# For the raw .joblib file:\n",
    "display(FileLink('hubspot_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab05c065-b77f-4cb2-99d4-8d124b41a0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Hubspot Data Sample:\n",
      "                                HubSpot_ID                   CreatedOn  \\\n",
      "0  HS-1c5e17ac-28ad-4771-b385-62a710336f69  2024-08-27T21:03:33.090348   \n",
      "1  HS-451b47c1-0ed0-4da4-93a4-f1b3a8ebf219  2025-01-03T04:35:43.780510   \n",
      "2  HS-f2ed3f7c-a4ca-40f1-aba3-a2ca6ba1bf6e  2024-07-10T06:11:38.096262   \n",
      "3  HS-54afff7f-a327-4fb5-a8b9-60b9c036d567  2025-01-04T15:03:45.793348   \n",
      "4  HS-526b530f-5c09-407d-9c33-673fd4a516e2  2024-11-01T23:27:35.954706   \n",
      "\n",
      "  FirstName  LastName                   Email            LifecycleStage  \\\n",
      "0     Molly  Mitchell     lrhodes@example.net                      Lead   \n",
      "1  Kimberly   Krueger   matthew74@example.net               Opportunity   \n",
      "2    Steven    Oliver    bonnie60@example.org  Marketing Qualified Lead   \n",
      "3      Mark      Rush    randrews@example.net      Sales Qualified Lead   \n",
      "4       Amy    Chaney  colinblake@example.org                      Lead   \n",
      "\n",
      "                         Website  LeadScore  \n",
      "0  https://edwards-robinson.biz/         39  \n",
      "1       http://www.gonzalez.com/         15  \n",
      "2             http://conrad.biz/          9  \n",
      "3        http://www.bullock.com/          9  \n",
      "4            https://davies.com/         91  \n",
      "\n",
      "Encoded Client Features Sample:\n",
      "   LifecycleStage_Customer  LifecycleStage_Lead  \\\n",
      "0                    False                 True   \n",
      "1                    False                False   \n",
      "2                    False                False   \n",
      "3                    False                False   \n",
      "4                    False                 True   \n",
      "\n",
      "   LifecycleStage_Marketing Qualified Lead  LifecycleStage_Opportunity  \\\n",
      "0                                    False                       False   \n",
      "1                                    False                        True   \n",
      "2                                     True                       False   \n",
      "3                                    False                       False   \n",
      "4                                    False                       False   \n",
      "\n",
      "   LifecycleStage_Sales Qualified Lead  LifecycleStage_Subscriber  LeadScore  \n",
      "0                                False                      False         39  \n",
      "1                                False                      False         15  \n",
      "2                                False                      False          9  \n",
      "3                                 True                      False          9  \n",
      "4                                False                      False         91  \n",
      "\n",
      "Columns from training (hubspot_columns.json):\n",
      "['LifecycleStage_Customer', 'LifecycleStage_Lead', 'LifecycleStage_Marketing Qualified Lead', 'LifecycleStage_Opportunity', 'LifecycleStage_Sales Qualified Lead', 'LifecycleStage_Subscriber', 'LeadScore']\n",
      "\n",
      "Reindexed Encoded Features (should have 7 columns):\n",
      "   LifecycleStage_Customer  LifecycleStage_Lead  \\\n",
      "0                    False                 True   \n",
      "1                    False                False   \n",
      "2                    False                False   \n",
      "3                    False                False   \n",
      "4                    False                 True   \n",
      "\n",
      "   LifecycleStage_Marketing Qualified Lead  LifecycleStage_Opportunity  \\\n",
      "0                                    False                       False   \n",
      "1                                    False                        True   \n",
      "2                                     True                       False   \n",
      "3                                    False                       False   \n",
      "4                                    False                       False   \n",
      "\n",
      "   LifecycleStage_Sales Qualified Lead  LifecycleStage_Subscriber  LeadScore  \n",
      "0                                False                      False         39  \n",
      "1                                False                      False         15  \n",
      "2                                False                      False          9  \n",
      "3                                 True                      False          9  \n",
      "4                                False                      False         91  \n",
      "\n",
      "Predictions on Client Hubspot Data:\n",
      "                                HubSpot_ID            LifecycleStage  \\\n",
      "0  HS-1c5e17ac-28ad-4771-b385-62a710336f69                      Lead   \n",
      "1  HS-451b47c1-0ed0-4da4-93a4-f1b3a8ebf219               Opportunity   \n",
      "2  HS-f2ed3f7c-a4ca-40f1-aba3-a2ca6ba1bf6e  Marketing Qualified Lead   \n",
      "3  HS-54afff7f-a327-4fb5-a8b9-60b9c036d567      Sales Qualified Lead   \n",
      "4  HS-526b530f-5c09-407d-9c33-673fd4a516e2                      Lead   \n",
      "\n",
      "   LeadScore  Predicted_Churn  \n",
      "0         39                0  \n",
      "1         15                1  \n",
      "2          9                1  \n",
      "3          9                1  \n",
      "4         91                0  \n",
      "\n",
      "Saved predictions to hubspot_data_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Download client's Hubspot data from S3\n",
    "# -------------------------------\n",
    "bucket = \"my-churnshield-data\"\n",
    "key = \"Data Sources/hubspot_data.csv\"  \n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=\"us-east-2\")\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "data = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "df_clients = pd.read_csv(StringIO(data))\n",
    "print(\"Client Hubspot Data Sample:\")\n",
    "print(df_clients.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Preprocess the data to extract features\n",
    "# -------------------------------\n",
    "# For Hubspot, we'll use 'LifecycleStage' (categorical) and 'LeadScore' (numeric) as features.\n",
    "features = df_clients[[\"LifecycleStage\", \"LeadScore\"]]\n",
    "\n",
    "# One-hot encode the 'LifecycleStage' column; keep 'LeadScore' numeric.\n",
    "lifecycle_encoded = pd.get_dummies(features[\"LifecycleStage\"], prefix=\"LifecycleStage\")\n",
    "features_encoded = pd.concat([lifecycle_encoded, features[\"LeadScore\"]], axis=1)\n",
    "print(\"\\nEncoded Client Features Sample:\")\n",
    "print(features_encoded.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load the training columns from JSON\n",
    "# -------------------------------\n",
    "# This JSON file was created during training, containing the exact dummy columns.\n",
    "with open(\"hubspot_columns.json\", \"r\") as f:\n",
    "    expected_columns = json.load(f)\n",
    "\n",
    "print(\"\\nColumns from training (hubspot_columns.json):\")\n",
    "print(expected_columns)\n",
    "\n",
    "# Reindex the encoded DataFrame to have the expected columns, filling missing columns with 0\n",
    "features_encoded = features_encoded.reindex(columns=expected_columns, fill_value=0)\n",
    "print(\"\\nReindexed Encoded Features (should have\", len(expected_columns), \"columns):\")\n",
    "print(features_encoded.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Load the trained Hubspot-specific model\n",
    "# -------------------------------\n",
    "model = joblib.load(\"hubspot_model.joblib\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Generate churn predictions\n",
    "# -------------------------------\n",
    "predictions = model.predict(features_encoded)\n",
    "df_clients[\"Predicted_Churn\"] = predictions\n",
    "\n",
    "# Display a few predictions alongside key columns\n",
    "print(\"\\nPredictions on Client Hubspot Data:\")\n",
    "print(df_clients[[\"HubSpot_ID\", \"LifecycleStage\", \"LeadScore\", \"Predicted_Churn\"]].head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Save the DataFrame with predictions to a new CSV file\n",
    "# -------------------------------\n",
    "output_file = \"hubspot_data_with_predictions.csv\"\n",
    "df_clients.to_csv(output_file, index=False)\n",
    "print(\"\\nSaved predictions to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be9f2c5b-1635-407d-9473-0968ef693e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Configuration for Hubspot predictions\n",
    "BUCKET = \"my-churnshield-data\"\n",
    "KEY = \"Predictions/hubspot_data_with_predictions.csv\"  \n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        # Create an S3 client in the correct region\n",
    "        s3_client = boto3.client(\"s3\", region_name=\"us-east-2\")\n",
    "        \n",
    "        # Retrieve the CSV file from S3\n",
    "        obj = s3_client.get_object(Bucket=BUCKET, Key=KEY)\n",
    "        data = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "        \n",
    "        # Load the CSV data into a pandas DataFrame\n",
    "        df = pd.read_csv(StringIO(data))\n",
    "        \n",
    "        # Convert the DataFrame into a list of dictionaries\n",
    "        predictions = df.to_dict(orient=\"records\")\n",
    "        \n",
    "        # Return the predictions as a JSON response with appropriate headers\n",
    "        return {\n",
    "            \"statusCode\": 200,\n",
    "            \"body\": json.dumps({\n",
    "                \"predictions\": predictions\n",
    "            }),\n",
    "            \"headers\": {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Access-Control-Allow-Origin\": \"*\"  \n",
    "            }\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"statusCode\": 500,\n",
    "            \"body\": json.dumps({\"error\": str(e)}),\n",
    "            \"headers\": {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Access-Control-Allow-Origin\": \"*\"\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747248d-96a6-487c-b182-1188e4026681",
   "metadata": {},
   "source": [
    "## Zendesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a658d968-2b90-4068-8296-f1c4bdf0860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the VADER lexicon\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cc3310a-8c72-43f5-afed-77e5efcf13d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Zendesk data with sentiment-based churn created: zendesk_data_synthetic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader.lexicon: Package 'vader.lexicon' not\n",
      "[nltk_data]     found in index\n"
     ]
    }
   ],
   "source": [
    "## CREATING SYNTHETIC DATA\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "from faker import Faker\n",
    "from datetime import timedelta\n",
    "import nltk\n",
    "nltk.download('vader.lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from io import StringIO\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "num_rows = 100\n",
    "statuses = [\"Open\", \"Solved\", \"New\", \"Pending\", \"Closed\"]\n",
    "priorities = [\"Low\", \"Normal\", \"High\", \"Urgent\"]\n",
    "\n",
    "def generate_random_sentence():\n",
    "    \"\"\"\n",
    "    Generate a short random sentence using Faker for more variety.\n",
    "    Can also inject specific negative/positive words.\n",
    "    \"\"\"\n",
    "    # We'll just use a couple of random sentences from Faker\n",
    "    sentence1 = fake.sentence(nb_words=6)\n",
    "    sentence2 = fake.sentence(nb_words=6)\n",
    "    return sentence1 + \" \" + sentence2\n",
    "\n",
    "synthetic_data = []\n",
    "for _ in range(num_rows):\n",
    "    ticket_id = \"ZD-\" + str(uuid.uuid4())[:8]\n",
    "    \n",
    "    # Random submitted time in the last 2 years\n",
    "    submitted_at = fake.date_time_between(start_date=\"-2y\", end_date=\"now\")\n",
    "    # updated_at is some random time after submitted_at (up to 30 days)\n",
    "    updated_at = submitted_at + timedelta(days=random.randint(0, 30))\n",
    "\n",
    "    user_id = \"USER-\" + str(random.randint(1000, 9999))\n",
    "    \n",
    "    # Generate subject + description\n",
    "    subject_text = generate_random_sentence()\n",
    "    desc_text = generate_random_sentence()\n",
    "\n",
    "    status = random.choice(statuses)\n",
    "    priority = random.choice(priorities)\n",
    "    agent_name = fake.name()\n",
    "\n",
    "    # Combine subject and description for sentiment analysis\n",
    "    combined_text = subject_text + \" \" + desc_text\n",
    "    \n",
    "    # Use VADER to get sentiment scores\n",
    "    scores = sid.polarity_scores(combined_text)\n",
    "    compound_score = scores['compound']\n",
    "    \n",
    "    # Decide churn based on compound score\n",
    "    # Can adjust threshold to preference (e.g., -0.2, -0.1, etc.)\n",
    "    if compound_score < -0:\n",
    "        churn = 1\n",
    "    else:\n",
    "        churn = 0\n",
    "\n",
    "    synthetic_data.append([\n",
    "        ticket_id,\n",
    "        submitted_at.isoformat(),\n",
    "        updated_at.isoformat(),\n",
    "        user_id,\n",
    "        subject_text,\n",
    "        desc_text,\n",
    "        status,\n",
    "        priority,\n",
    "        agent_name,\n",
    "        churn\n",
    "    ])\n",
    "\n",
    "columns = [\n",
    "    \"Ticket_ID\", \"SubmittedAt\", \"UpdatedAt\", \"UserID\",\n",
    "    \"TicketSubject\", \"TicketDescription\", \"Status\", \"Priority\", \"Agent\",\n",
    "    \"Churn\"\n",
    "]\n",
    "\n",
    "df_synthetic = pd.DataFrame(synthetic_data, columns=columns)\n",
    "df_synthetic.to_csv(\"zendesk_data_synthetic.csv\", index=False)\n",
    "print(\"Synthetic Zendesk data with sentiment-based churn created: zendesk_data_synthetic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdbe8947-c68c-4c60-b459-fa65ac64cabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zendesk Data Sample:\n",
      "     Ticket_ID                 SubmittedAt                   UpdatedAt  \\\n",
      "0  ZD-9e8d9cc8  2024-10-27T20:38:32.780454  2024-11-25T20:38:32.780454   \n",
      "1  ZD-9831a2e7  2024-12-11T05:53:12.712889  2024-12-27T05:53:12.712889   \n",
      "2  ZD-002ee304  2024-12-09T23:32:20.494166  2025-01-03T23:32:20.494166   \n",
      "3  ZD-b9194fdd  2024-08-10T10:38:10.863898  2024-08-17T10:38:10.863898   \n",
      "4  ZD-17615042  2024-09-04T11:51:36.325238  2024-09-18T11:51:36.325238   \n",
      "\n",
      "      UserID                                      TicketSubject  \\\n",
      "0  USER-2163  Affect change evidence and sing table. Electio...   \n",
      "1  USER-3743  Section off everything turn artist tell. Study...   \n",
      "2  USER-6612  Try school still one reality politics rest. Wo...   \n",
      "3  USER-4100  Contain popular one open better lot someone. P...   \n",
      "4  USER-1748  Dinner past long cup purpose hot. Nature assum...   \n",
      "\n",
      "                                   TicketDescription   Status Priority  \\\n",
      "0  Artist drug huge room under. Itself guess of m...     Open   Urgent   \n",
      "1  Some small develop by success live alone. Town...      New   Normal   \n",
      "2  Happen if agree six more mind also. Add happen...   Closed      Low   \n",
      "3  Create for man she enough. Increase while defe...      New     High   \n",
      "4  Money wrong campaign year court. Teach get edg...  Pending      Low   \n",
      "\n",
      "              Agent  Churn  \n",
      "0     Bradley Hicks      0  \n",
      "1   Alexander Blair      0  \n",
      "2      Emily Miller      0  \n",
      "3  Allison Williams      0  \n",
      "4  Monique Johnston      1  \n",
      "\n",
      "Feature Sample:\n",
      "   CompoundScore\n",
      "0         0.5574\n",
      "1         0.4019\n",
      "2         0.2960\n",
      "3         0.9022\n",
      "4        -0.1027\n",
      "Training columns: ['CompoundScore']\n",
      "\n",
      "Model Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n",
      "\n",
      "Zendesk model saved to zendesk_model.joblib\n"
     ]
    }
   ],
   "source": [
    "## TRAINING MODEL\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Load synthetic Zendesk data from CSV\n",
    "df = pd.read_csv(\"zendesk_data_synthetic.csv\")\n",
    "print(\"Zendesk Data Sample:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Preprocess: Combine TicketSubject and TicketDescription for sentiment analysis\n",
    "df[\"CombinedText\"] = df[\"TicketSubject\"] + \" \" + df[\"TicketDescription\"]\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Compute the compound sentiment score for each ticket\n",
    "df[\"CompoundScore\"] = df[\"CombinedText\"].apply(lambda text: sid.polarity_scores(text)[\"compound\"])\n",
    "\n",
    "# For modeling, we'll use the CompoundScore as the single feature.\n",
    "features = df[[\"CompoundScore\"]]\n",
    "target = df[\"Churn\"]\n",
    "\n",
    "print(\"\\nFeature Sample:\")\n",
    "print(features.head())\n",
    "\n",
    "# 3. Save the training columns for later inference\n",
    "# (In this case, it will simply be [\"CompoundScore\"])\n",
    "training_columns = list(features.columns)\n",
    "with open(\"zendesk_columns.json\", \"w\") as f:\n",
    "    json.dump(training_columns, f)\n",
    "print(\"Training columns:\", training_columns)\n",
    "\n",
    "# 4. Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Train a RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nModel Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 7. Save the trained model to a file for later use\n",
    "joblib.dump(clf, \"zendesk_model.joblib\")\n",
    "print(\"\\nZendesk model saved to zendesk_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d502dba-0048-464b-b6ed-50ce8bbe6bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='zendesk_model.joblib' target='_blank'>zendesk_model.joblib</a><br>"
      ],
      "text/plain": [
       "/home/sagemaker-user/zendesk_model.joblib"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# For the raw .joblib file:\n",
    "display(FileLink('zendesk_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c4783e1-8a5e-4ee0-b452-3826c893b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Zendesk Data Sample:\n",
      "                                 Ticket_ID                 SubmittedAt  \\\n",
      "0  ZD-b59bc9dc-a911-4ed5-b286-47dcd465d527  2024-07-03T05:21:11.206803   \n",
      "1  ZD-7bec3e64-dad6-45f2-8b95-f12d6fed915a  2024-12-08T22:54:35.805105   \n",
      "2  ZD-cf900bc2-7663-43fc-b425-4e2f7611127c  2024-05-07T00:16:50.778418   \n",
      "3  ZD-0d561782-be40-4f1d-afe8-f5a0dd95a32b  2025-01-12T16:50:56.347338   \n",
      "4  ZD-1d6c76a9-4761-4ec0-b020-36b127a36181  2024-09-03T14:44:16.091109   \n",
      "\n",
      "                    UpdatedAt     UserID  \\\n",
      "0  2024-07-03T05:21:11.206803  USER-7191   \n",
      "1  2025-01-05T22:54:35.805105  USER-4471   \n",
      "2  2024-05-13T00:16:50.778418  USER-7103   \n",
      "3  2025-01-26T16:50:56.347338  USER-6479   \n",
      "4  2024-09-06T14:44:16.091109  USER-9602   \n",
      "\n",
      "                           TicketSubject  \\\n",
      "0  Trouble box center ok accept science.   \n",
      "1      Always attorney individual scene.   \n",
      "2            How safe so claim left own.   \n",
      "3            Imagine face evidence road.   \n",
      "4  Direction leave technology important.   \n",
      "\n",
      "                                   TicketDescription  Status Priority  \\\n",
      "0    Worry foot back whatever recently police civil.    Open     High   \n",
      "1  Matter mission must event. Four well member se...  Solved      Low   \n",
      "2                             Will wait today begin.  Solved   Normal   \n",
      "3  Positive hour gas require me watch boy. Despit...    Open   Normal   \n",
      "4  Understand area seek. Accept soon whose finish...    Open     High   \n",
      "\n",
      "                  Agent  \n",
      "0         Laura Edwards  \n",
      "1       Michael Harrell  \n",
      "2  Alejandra Fitzgerald  \n",
      "3             Amy Ponce  \n",
      "4         Frank Nichols  \n",
      "\n",
      "Encoded Zendesk Features Sample:\n",
      "   CompoundScore\n",
      "0        -0.2023\n",
      "1         0.5994\n",
      "2         0.4404\n",
      "3         0.5574\n",
      "4         0.4939\n",
      "\n",
      "Columns from training (zendesk_columns.json):\n",
      "['CompoundScore']\n",
      "\n",
      "Reindexed Encoded Features (should have 1 columns):\n",
      "   CompoundScore\n",
      "0        -0.2023\n",
      "1         0.5994\n",
      "2         0.4404\n",
      "3         0.5574\n",
      "4         0.4939\n",
      "\n",
      "Predictions on Client Zendesk Data:\n",
      "                                 Ticket_ID  \\\n",
      "0  ZD-b59bc9dc-a911-4ed5-b286-47dcd465d527   \n",
      "1  ZD-7bec3e64-dad6-45f2-8b95-f12d6fed915a   \n",
      "2  ZD-cf900bc2-7663-43fc-b425-4e2f7611127c   \n",
      "3  ZD-0d561782-be40-4f1d-afe8-f5a0dd95a32b   \n",
      "4  ZD-1d6c76a9-4761-4ec0-b020-36b127a36181   \n",
      "\n",
      "                           TicketSubject  \\\n",
      "0  Trouble box center ok accept science.   \n",
      "1      Always attorney individual scene.   \n",
      "2            How safe so claim left own.   \n",
      "3            Imagine face evidence road.   \n",
      "4  Direction leave technology important.   \n",
      "\n",
      "                                   TicketDescription  Predicted_Churn  \n",
      "0    Worry foot back whatever recently police civil.                1  \n",
      "1  Matter mission must event. Four well member se...                0  \n",
      "2                             Will wait today begin.                0  \n",
      "3  Positive hour gas require me watch boy. Despit...                0  \n",
      "4  Understand area seek. Accept soon whose finish...                0  \n",
      "\n",
      "Saved predictions to zendesk_data_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "## Creating predictions for Zendesk\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import joblib\n",
    "import json\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Download client's Zendesk data from S3\n",
    "# -------------------------------\n",
    "bucket = \"my-churnshield-data\"\n",
    "key = \"Data Sources/zendesk_data.csv\" \n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=\"us-east-2\")\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "data = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "df_clients = pd.read_csv(StringIO(data))\n",
    "print(\"Client Zendesk Data Sample:\")\n",
    "print(df_clients.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Preprocess data to extract features\n",
    "# -------------------------------\n",
    "# Combine TicketSubject and TicketDescription for sentiment analysis\n",
    "df_clients[\"CombinedText\"] = df_clients[\"TicketSubject\"] + \" \" + df_clients[\"TicketDescription\"]\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Compute the compound sentiment score for each ticket\n",
    "df_clients[\"CompoundScore\"] = df_clients[\"CombinedText\"].apply(lambda text: sid.polarity_scores(text)[\"compound\"])\n",
    "\n",
    "# For modeling, we'll use \"CompoundScore\" as our feature\n",
    "features_encoded = df_clients[[\"CompoundScore\"]]\n",
    "print(\"\\nEncoded Zendesk Features Sample:\")\n",
    "print(features_encoded.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load the training columns from JSON\n",
    "# -------------------------------\n",
    "# This file should contain the exact feature columns used during training (e.g., [\"CompoundScore\"])\n",
    "with open(\"zendesk_columns.json\", \"r\") as f:\n",
    "    expected_columns = json.load(f)\n",
    "\n",
    "print(\"\\nColumns from training (zendesk_columns.json):\")\n",
    "print(expected_columns)\n",
    "\n",
    "# Reindex the DataFrame to match the expected columns\n",
    "features_encoded = features_encoded.reindex(columns=expected_columns, fill_value=0)\n",
    "print(\"\\nReindexed Encoded Features (should have\", len(expected_columns), \"columns):\")\n",
    "print(features_encoded.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Load the trained Zendesk-specific model\n",
    "# -------------------------------\n",
    "model = joblib.load(\"zendesk_model.joblib\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Generate churn predictions\n",
    "# -------------------------------\n",
    "predictions = model.predict(features_encoded)\n",
    "df_clients[\"Predicted_Churn\"] = predictions\n",
    "\n",
    "# Display a few predictions alongside key columns\n",
    "print(\"\\nPredictions on Client Zendesk Data:\")\n",
    "print(df_clients[[\"Ticket_ID\", \"TicketSubject\", \"TicketDescription\", \"Predicted_Churn\"]].head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Save the DataFrame with predictions to a new CSV file\n",
    "# -------------------------------\n",
    "output_file = \"zendesk_data_with_predictions.csv\"\n",
    "df_clients.to_csv(output_file, index=False)\n",
    "print(\"\\nSaved predictions to\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738cd26-c6e8-4e98-8d13-fa21c0de8af9",
   "metadata": {},
   "source": [
    "## Billing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9c08a40-96f5-446d-8cce-7ca52bddad34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Billing data created: billing_data_synthetic.csv\n"
     ]
    }
   ],
   "source": [
    "## CREATING SYNTHETIC DATA\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "num_rows = 100\n",
    "\n",
    "# Possible subscription plans\n",
    "subscription_plans = [\"Basic\", \"Standard\", \"Premium\", \"Enterprise\"]\n",
    "\n",
    "# Payment methods and statuses\n",
    "payment_methods = [\"Credit Card\", \"Wire Transfer\", \"PayPal\"]\n",
    "payment_statuses = [\"Completed\", \"Pending\", \"Failed\"]\n",
    "\n",
    "synthetic_data = []\n",
    "\n",
    "for _ in range(num_rows):\n",
    "    billing_id = \"BILL-\" + str(uuid.uuid4())[:8]  # short random ID\n",
    "    user_id = \"USER-\" + str(random.randint(1000, 9999))\n",
    "\n",
    "    # Random subscription plan\n",
    "    subscription_plan = random.choice(subscription_plans)\n",
    "\n",
    "    # Generate a random amount, e.g. from 10 to 500\n",
    "    amount = round(random.uniform(10, 500), 2)\n",
    "\n",
    "    currency = \"USD\"\n",
    "\n",
    "    # Random transaction date in the last 2 years\n",
    "    transaction_date = fake.date_time_between(start_date=\"-2y\", end_date=\"now\").isoformat()\n",
    "\n",
    "    # Random payment method and status\n",
    "    payment_method = random.choice(payment_methods)\n",
    "    payment_status = random.choice(payment_statuses)\n",
    "\n",
    "    # ----- Churn Logic -----\n",
    "    # Example approach:\n",
    "    #  - If payment_status == \"Failed\" => high chance churn=1\n",
    "    #  - If payment_status == \"Completed\" => likely churn=0\n",
    "    #  - If subscription_plan == \"Basic\" => slightly higher chance of churn\n",
    "    #  - Otherwise random\n",
    "\n",
    "    # Start with a base churn probability\n",
    "    churn_prob = 0.2\n",
    "\n",
    "    # Increase probability if subscription plan is Basic\n",
    "    if subscription_plan == \"Basic\":\n",
    "        churn_prob += 0.2\n",
    "\n",
    "    # Payment status weighting\n",
    "    if payment_status == \"Failed\":\n",
    "        churn_prob += 0.5  # Much higher chance\n",
    "    elif payment_status == \"Completed\":\n",
    "        churn_prob -= 0.1  # Less chance\n",
    "\n",
    "    # Cap churn_prob between 0 and 1\n",
    "    churn_prob = min(max(churn_prob, 0), 1)\n",
    "\n",
    "    # Decide churn based on final churn_prob\n",
    "    churn = 1 if random.random() < churn_prob else 0\n",
    "\n",
    "    synthetic_data.append([\n",
    "        billing_id,\n",
    "        user_id,\n",
    "        subscription_plan,\n",
    "        amount,\n",
    "        currency,\n",
    "        transaction_date,\n",
    "        payment_method,\n",
    "        payment_status,\n",
    "        churn\n",
    "    ])\n",
    "\n",
    "columns = [\n",
    "    \"Billing_ID\",\n",
    "    \"UserID\",\n",
    "    \"SubscriptionPlan\",\n",
    "    \"Amount\",\n",
    "    \"Currency\",\n",
    "    \"TransactionDate\",\n",
    "    \"PaymentMethod\",\n",
    "    \"PaymentStatus\",\n",
    "    \"Churn\"\n",
    "]\n",
    "\n",
    "df_synthetic = pd.DataFrame(synthetic_data, columns=columns)\n",
    "df_synthetic.to_csv(\"billing_data_synthetic.csv\", index=False)\n",
    "print(\"Synthetic Billing data created: billing_data_synthetic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8614e6e-cf9b-45ff-aa1c-fce24aaf9a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billing Data Sample:\n",
      "      Billing_ID     UserID SubscriptionPlan  Amount Currency  \\\n",
      "0  BILL-c7053b74  USER-2130       Enterprise   71.16      USD   \n",
      "1  BILL-f6fe7e56  USER-2625          Premium  324.95      USD   \n",
      "2  BILL-dc5ebe15  USER-2433         Standard  277.07      USD   \n",
      "3  BILL-11e27330  USER-5514          Premium  103.73      USD   \n",
      "4  BILL-2c1d0e98  USER-2412          Premium  481.60      USD   \n",
      "\n",
      "              TransactionDate  PaymentMethod PaymentStatus  Churn  \n",
      "0  2024-07-15T17:09:28.759243    Credit Card       Pending      1  \n",
      "1  2023-12-24T10:07:01.570484  Wire Transfer        Failed      0  \n",
      "2  2024-09-24T16:10:16.351231    Credit Card     Completed      0  \n",
      "3  2023-06-04T08:35:06.576073    Credit Card     Completed      0  \n",
      "4  2025-02-19T06:57:55.954580  Wire Transfer       Pending      0  \n",
      "\n",
      "Encoded Features Sample:\n",
      "   Plan_Basic  Plan_Enterprise  Plan_Premium  Plan_Standard  \\\n",
      "0       False             True         False          False   \n",
      "1       False            False          True          False   \n",
      "2       False            False         False           True   \n",
      "3       False            False          True          False   \n",
      "4       False            False          True          False   \n",
      "\n",
      "   PayStatus_Completed  PayStatus_Failed  PayStatus_Pending  Amount  \n",
      "0                False             False               True   71.16  \n",
      "1                False              True              False  324.95  \n",
      "2                 True             False              False  277.07  \n",
      "3                 True             False              False  103.73  \n",
      "4                False             False               True  481.60  \n",
      "Training columns: ['Plan_Basic', 'Plan_Enterprise', 'Plan_Premium', 'Plan_Standard', 'PayStatus_Completed', 'PayStatus_Failed', 'PayStatus_Pending', 'Amount']\n",
      "\n",
      "Model Accuracy: 0.8\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.90      0.82        10\n",
      "           1       0.88      0.70      0.78        10\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.81      0.80      0.80        20\n",
      "weighted avg       0.81      0.80      0.80        20\n",
      "\n",
      "\n",
      "Billing model saved to billing_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Load synthetic Billing data from CSV\n",
    "df = pd.read_csv(\"billing_data_synthetic.csv\")\n",
    "print(\"Billing Data Sample:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Select features and target\n",
    "# We'll use 'SubscriptionPlan' (categorical), 'PaymentStatus' (categorical), and 'Amount' (numeric).\n",
    "features = df[[\"SubscriptionPlan\", \"PaymentStatus\", \"Amount\"]]\n",
    "target = df[\"Churn\"]\n",
    "\n",
    "# 3. One-hot encode the categorical columns\n",
    "subscription_encoded = pd.get_dummies(features[\"SubscriptionPlan\"], prefix=\"Plan\")\n",
    "payment_encoded = pd.get_dummies(features[\"PaymentStatus\"], prefix=\"PayStatus\")\n",
    "\n",
    "# Combine these with the numeric 'Amount' column\n",
    "features_encoded = pd.concat([subscription_encoded, payment_encoded, features[\"Amount\"]], axis=1)\n",
    "\n",
    "print(\"\\nEncoded Features Sample:\")\n",
    "print(features_encoded.head())\n",
    "\n",
    "# 4. Save the training columns for inference\n",
    "training_columns = list(features_encoded.columns)\n",
    "with open(\"billing_columns.json\", \"w\") as f:\n",
    "    json.dump(training_columns, f)\n",
    "print(\"Training columns:\", training_columns)\n",
    "\n",
    "# 5. Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_encoded, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. Train a RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nModel Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 8. Save the trained model to a file for later use\n",
    "joblib.dump(clf, \"billing_model.joblib\")\n",
    "print(\"\\nBilling model saved to billing_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e47a5df-9e15-4488-a1f0-8f1306ceab6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='billing_model.joblib' target='_blank'>billing_model.joblib</a><br>"
      ],
      "text/plain": [
       "/home/sagemaker-user/billing_model.joblib"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# For the raw .joblib file:\n",
    "display(FileLink('billing_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06986856-cec8-49cf-a75e-ce568d94ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Billing Data Sample:\n",
      "                                  Billing_ID     UserID SubscriptionPlan  \\\n",
      "0  BILL-27eba3ee-e298-4348-98ed-9fa8474ff727  USER-8359            Basic   \n",
      "1  BILL-6682bc84-8806-42b5-ab80-df08b7f5b5ca  USER-3216       Enterprise   \n",
      "2  BILL-c3d9c47b-032f-4c7e-8322-6549ee14b4c8  USER-5794       Enterprise   \n",
      "3  BILL-44884268-8956-44a1-ab2a-37fc9277f0c9  USER-6822       Enterprise   \n",
      "4  BILL-edecccfc-da79-423f-b17c-f8952bbd559f  USER-1518       Enterprise   \n",
      "\n",
      "   Amount Currency             TransactionDate  PaymentMethod PaymentStatus  \n",
      "0   30.39      USD  2024-12-28T20:41:39.615550  Wire Transfer       Pending  \n",
      "1  145.33      USD  2024-11-28T00:04:24.702024         PayPal        Failed  \n",
      "2  491.75      USD  2024-08-22T14:09:02.394898  Wire Transfer        Failed  \n",
      "3  185.83      USD  2024-03-22T16:31:45.238658    Credit Card       Pending  \n",
      "4  183.16      USD  2025-01-06T13:06:24.559926  Wire Transfer     Completed  \n",
      "\n",
      "Encoded Billing Features Sample:\n",
      "   Plan_Basic  Plan_Enterprise  Plan_Premium  Plan_Standard  \\\n",
      "0        True            False         False          False   \n",
      "1       False             True         False          False   \n",
      "2       False             True         False          False   \n",
      "3       False             True         False          False   \n",
      "4       False             True         False          False   \n",
      "\n",
      "   PayStatus_Completed  PayStatus_Failed  PayStatus_Pending  Amount  \n",
      "0                False             False               True   30.39  \n",
      "1                False              True              False  145.33  \n",
      "2                False              True              False  491.75  \n",
      "3                False             False               True  185.83  \n",
      "4                 True             False              False  183.16  \n",
      "\n",
      "Columns from training (billing_columns.json):\n",
      "['Plan_Basic', 'Plan_Enterprise', 'Plan_Premium', 'Plan_Standard', 'PayStatus_Completed', 'PayStatus_Failed', 'PayStatus_Pending', 'Amount']\n",
      "\n",
      "Reindexed Encoded Features (should have 8 columns):\n",
      "   Plan_Basic  Plan_Enterprise  Plan_Premium  Plan_Standard  \\\n",
      "0        True            False         False          False   \n",
      "1       False             True         False          False   \n",
      "2       False             True         False          False   \n",
      "3       False             True         False          False   \n",
      "4       False             True         False          False   \n",
      "\n",
      "   PayStatus_Completed  PayStatus_Failed  PayStatus_Pending  Amount  \n",
      "0                False             False               True   30.39  \n",
      "1                False              True              False  145.33  \n",
      "2                False              True              False  491.75  \n",
      "3                False             False               True  185.83  \n",
      "4                 True             False              False  183.16  \n",
      "\n",
      "Predictions on Client Billing Data:\n",
      "                                  Billing_ID SubscriptionPlan  Amount  \\\n",
      "0  BILL-27eba3ee-e298-4348-98ed-9fa8474ff727            Basic   30.39   \n",
      "1  BILL-6682bc84-8806-42b5-ab80-df08b7f5b5ca       Enterprise  145.33   \n",
      "2  BILL-c3d9c47b-032f-4c7e-8322-6549ee14b4c8       Enterprise  491.75   \n",
      "3  BILL-44884268-8956-44a1-ab2a-37fc9277f0c9       Enterprise  185.83   \n",
      "4  BILL-edecccfc-da79-423f-b17c-f8952bbd559f       Enterprise  183.16   \n",
      "\n",
      "  PaymentStatus  Predicted_Churn  \n",
      "0       Pending                0  \n",
      "1        Failed                1  \n",
      "2        Failed                1  \n",
      "3       Pending                0  \n",
      "4     Completed                0  \n",
      "\n",
      "Saved predictions to billing_data_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Download client's Billing data from S3\n",
    "# -------------------------------\n",
    "bucket = \"my-churnshield-data\"\n",
    "key = \"Data Sources/billing_data.csv\"  # Adjust if your file name/path is different\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=\"us-east-2\")\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "data = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "df_clients = pd.read_csv(StringIO(data))\n",
    "print(\"Client Billing Data Sample:\")\n",
    "print(df_clients.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Preprocess data to extract features\n",
    "# -------------------------------\n",
    "# We have 'SubscriptionPlan' (categorical), 'PaymentStatus' (categorical), and 'Amount' (numeric).\n",
    "subscription_encoded = pd.get_dummies(df_clients[\"SubscriptionPlan\"], prefix=\"Plan\")\n",
    "payment_encoded = pd.get_dummies(df_clients[\"PaymentStatus\"], prefix=\"PayStatus\")\n",
    "\n",
    "# Combine the encoded columns with the numeric 'Amount' column\n",
    "features_encoded = pd.concat([subscription_encoded, payment_encoded, df_clients[\"Amount\"]], axis=1)\n",
    "print(\"\\nEncoded Billing Features Sample:\")\n",
    "print(features_encoded.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load the training columns from JSON\n",
    "# -------------------------------\n",
    "# This file should contain the exact feature columns used during training \n",
    "# (e.g., [\"Plan_Basic\", \"Plan_Standard\", ..., \"PayStatus_Completed\", ..., \"Amount\"])\n",
    "with open(\"billing_columns.json\", \"r\") as f:\n",
    "    expected_columns = json.load(f)\n",
    "\n",
    "print(\"\\nColumns from training (billing_columns.json):\")\n",
    "print(expected_columns)\n",
    "\n",
    "# Reindex the DataFrame to match the expected columns\n",
    "features_encoded = features_encoded.reindex(columns=expected_columns, fill_value=0)\n",
    "print(\"\\nReindexed Encoded Features (should have\", len(expected_columns), \"columns):\")\n",
    "print(features_encoded.head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Load the trained Billing-specific model\n",
    "# -------------------------------\n",
    "model = joblib.load(\"billing_model.joblib\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Generate churn predictions\n",
    "# -------------------------------\n",
    "predictions = model.predict(features_encoded)\n",
    "df_clients[\"Predicted_Churn\"] = predictions\n",
    "\n",
    "# Display a few predictions alongside key columns\n",
    "print(\"\\nPredictions on Client Billing Data:\")\n",
    "print(df_clients[[\"Billing_ID\", \"SubscriptionPlan\", \"Amount\", \"PaymentStatus\", \"Predicted_Churn\"]].head())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Save the DataFrame with predictions to a new CSV file\n",
    "# -------------------------------\n",
    "output_file = \"billing_data_with_predictions.csv\"\n",
    "df_clients.to_csv(output_file, index=False)\n",
    "print(\"\\nSaved predictions to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b6f9a-4d1f-4d2a-a0c6-b41b785b8db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
